(window.webpackJsonp=window.webpackJsonp||[]).push([[52],{197:function(e,t,o){"use strict";o.r(t),t.default=[{title:"Introducing QA-Board: R&D Platform#What you still need   to integrate with qatools",type:0,sectionRef:"#",url:"blog/2019/04/24/introducing-qatools",content:""},{title:"What is qatools made of?",type:1,pageTitle:"Introducing QA-Board: R&D Platform#What you still need   to integrate with qatools",url:"blog/2019/04/24/introducing-qatools#what-is-qatools-made-of",content:"a client library..a web application that displays results. Icon? Maybe ask samsung design. Chekc github how they do it.... papillon (spent a lot of effort preparing), chrysalidediamond (too common?)bar char (too simple)boxplot (hard to recognize? geek? narrow)caniche (looking good?)colorful fish, bird.Etoile de mer? Check: CometML docs, polyaxon, MLflow, NNI microsoft  We are very happy to introduce qatools to help algorithm enginneers build products with powerful QA tools. We created qatools for the following reasons: Focus on experimenting algorithms, not worrying about the QA infrastructure.Most comparable tools focus on training for machine learning. Our use cases revolve around qualitative outputs. Hence, we feature advanced visualizations: out-of-the-box viewers for images, pointclouds, plotly, HTML, etc.Make it easy for engineers and QA to compare different algorithms, configurations, perform tuning, and share their results. And, finally, to provide a consistent look and feel across QA for all of our algorithms projects. "},{title:"How does Docusaurus work?",type:1,pageTitle:"Introducing QA-Board: R&D Platform#What you still need   to integrate with qatools",url:"blog/2019/04/24/introducing-qatools#how-does-docusaurus-work",content:"https://docusaurus.io/blog/2017/12/14/introducing-docusaurus "},{title:"qatools may not be a great fit",type:1,pageTitle:"Introducing QA-Board: R&D Platform#What you still need   to integrate with qatools",url:"blog/2019/04/24/introducing-qatools#qatools-may-not-be-a-great-fit",content:"You want reporting, integraton with gitNeed Shared and persistent database of results. Tensorboard is temporaryNeed to handle multiple projectsDoing low-level optimization or hardware design: you need to worry about (1) bit-accuracy of the new code (2) tools to understand your tradeoffs  "},{title:"qatools may not be a great fit for you if",type:1,pageTitle:"Introducing QA-Board: R&D Platform#What you still need   to integrate with qatools",url:"blog/2019/04/24/introducing-qatools#qatools-may-not-be-a-great-fit-for-you-if",content:"It is a step above notebooks you need to manage long training sessions, and need live training feedbackyou #What you still need to integrate with qatools task-runner infrastructure: internally we use X. TODO: sample Celery integration.deployment infrastructuremodel and database versionningCI: qatools doesn't do automation, triggers... Your CI should call it. "},{title:"Usage",type:1,pageTitle:"Introducing QA-Board: R&D Platform#What you still need   to integrate with qatools",url:"blog/2019/04/24/introducing-qatools#usage",content:"Follow the tutorial. How does it work, concretely? You need to write a run() python function that wraps your code. It gives you a CLI interface (eg qa run --input my_input) for local development.A configuration file, qaboard.yaml, describes metadata like how to find your inputs, what outputs and metrics you expect, etc.... after init, print link to the project's integration page? need to start server first ? (docker commands), qa init should get the URL, unless we default to the hosted version. Still need stuff like git permissions to clone, project namespace... (maybe provide a deploy key, or setup as a 2FA application? maybe offer inputs, with default...)API if no results, None, just 200 empty list...Results are automatically "},{title:"open source tech blocker",type:1,pageTitle:"Introducing QA-Board: R&D Platform#What you still need   to integrate with qatools",url:"blog/2019/04/24/introducing-qatools#open-source-tech-blocker",content:"rename dvs/backend => qatools/frontend + qatools/backend; common-infrastrcture/qatools => qatools/clistorage: need to review linux/windows: offer eg s8, make it clearergithub integration, per-project repo host, permissions, integrations like user pictures...replace lsf with eg celery, make it pluggable (qatools[lsf]), so also need to carve it out of the server...grep arthurf, grep dvs, grep tof...unified devops: docker compose the 2 IIFS, the server, terminate https once... "},{title:"CI",type:1,pageTitle:"Introducing QA-Board: R&D Platform#What you still need   to integrate with qatools",url:"blog/2019/04/24/introducing-qatools#ci",content:'Copyssh ispq@ispq-vdibashexport GITLAB_RUNNER=1export GITLAB_PROJECT=qatoolsexport GITLAB_TOKEN="xxxxxxxx" Getting Started Installation=> readmeqa init CLI featuresqa run, how to find tests, run()recipies for the run function:command, docker, direct pythonplatform, android...qa batch, groups of testsqa check-bit-accuracy CI integration: qa get --input output_directory Visualizations Tuning from the web UIneeded: artifacts, define and savetuningoptimizationcustom env? docker? Guides working with sub-projectsbit-accuracy: custom inputsgenerating reports, exporting/re-using results: APIinputs metadata: computing custom metrics per testMachine learning: working on whole databasesCreating meta-benchmarks (TODO) Admin Guides Starting a qatools server / restarting the serverqatools\'s own CIdatabase backups create sample docs structure with empty filesadd pages with the wikiserve docs on https://qa/docsremove wiki (point to docs) '},{title:"#What you still need   to integrate with qatools",type:0,sectionRef:"#",url:"blog",content:""},{title:"What is qatools made of?",type:1,pageTitle:"#What you still need   to integrate with qatools",url:"blog#what-is-qatools-made-of",content:"a client library..a web application that displays results. Icon? Maybe ask samsung design. Chekc github how they do it.... papillon (spent a lot of effort preparing), chrysalidediamond (too common?)bar char (too simple)boxplot (hard to recognize? geek? narrow)caniche (looking good?)colorful fish, bird.Etoile de mer? Check: CometML docs, polyaxon, MLflow, NNI microsoft  We are very happy to introduce qatools to help algorithm enginneers build products with powerful QA tools. We created qatools for the following reasons: Focus on experimenting algorithms, not worrying about the QA infrastructure.Most comparable tools focus on training for machine learning. Our use cases revolve around qualitative outputs. Hence, we feature advanced visualizations: out-of-the-box viewers for images, pointclouds, plotly, HTML, etc.Make it easy for engineers and QA to compare different algorithms, configurations, perform tuning, and share their results. And, finally, to provide a consistent look and feel across QA for all of our algorithms projects. "},{title:"How does Docusaurus work?",type:1,pageTitle:"#What you still need   to integrate with qatools",url:"blog#how-does-docusaurus-work",content:"https://docusaurus.io/blog/2017/12/14/introducing-docusaurus "},{title:"qatools may not be a great fit",type:1,pageTitle:"#What you still need   to integrate with qatools",url:"blog#qatools-may-not-be-a-great-fit",content:"You want reporting, integraton with gitNeed Shared and persistent database of results. Tensorboard is temporaryNeed to handle multiple projectsDoing low-level optimization or hardware design: you need to worry about (1) bit-accuracy of the new code (2) tools to understand your tradeoffs  "},{title:"qatools may not be a great fit for you if",type:1,pageTitle:"#What you still need   to integrate with qatools",url:"blog#qatools-may-not-be-a-great-fit-for-you-if",content:"It is a step above notebooks you need to manage long training sessions, and need live training feedbackyou #What you still need to integrate with qatools task-runner infrastructure: internally we use X. TODO: sample Celery integration.deployment infrastructuremodel and database versionningCI: qatools doesn't do automation, triggers... Your CI should call it. "},{title:"Usage",type:1,pageTitle:"#What you still need   to integrate with qatools",url:"blog#usage",content:"Follow the tutorial. How does it work, concretely? You need to write a run() python function that wraps your code. It gives you a CLI interface (eg qa run --input my_input) for local development.A configuration file, qaboard.yaml, describes metadata like how to find your inputs, what outputs and metrics you expect, etc.... after init, print link to the project's integration page? need to start server first ? (docker commands), qa init should get the URL, unless we default to the hosted version. Still need stuff like git permissions to clone, project namespace... (maybe provide a deploy key, or setup as a 2FA application? maybe offer inputs, with default...)API if no results, None, just 200 empty list...Results are automatically "},{title:"open source tech blocker",type:1,pageTitle:"#What you still need   to integrate with qatools",url:"blog#open-source-tech-blocker",content:"rename dvs/backend => qatools/frontend + qatools/backend; common-infrastrcture/qatools => qatools/clistorage: need to review linux/windows: offer eg s8, make it clearergithub integration, per-project repo host, permissions, integrations like user pictures...replace lsf with eg celery, make it pluggable (qatools[lsf]), so also need to carve it out of the server...grep arthurf, grep dvs, grep tof...unified devops: docker compose the 2 IIFS, the server, terminate https once... "},{title:"CI",type:1,pageTitle:"#What you still need   to integrate with qatools",url:"blog#ci",content:'Copyssh ispq@ispq-vdibashexport GITLAB_RUNNER=1export GITLAB_PROJECT=qatoolsexport GITLAB_TOKEN="xxxxxxxx" Getting Started Installation=> readmeqa init CLI featuresqa run, how to find tests, run()recipies for the run function:command, docker, direct pythonplatform, android...qa batch, groups of testsqa check-bit-accuracy CI integration: qa get --input output_directory Visualizations Tuning from the web UIneeded: artifacts, define and savetuningoptimizationcustom env? docker? Guides working with sub-projectsbit-accuracy: custom inputsgenerating reports, exporting/re-using results: APIinputs metadata: computing custom metrics per testMachine learning: working on whole databasesCreating meta-benchmarks (TODO) Admin Guides Starting a qatools server / restarting the serverqatools\'s own CIdatabase backups create sample docs structure with empty filesadd pages with the wikiserve docs on https://qa/docsremove wiki (point to docs) '},{title:"Alternatives and missing features",type:0,sectionRef:"#",url:"docs/alternatives-and-missing-features",content:""},{title:"Missing features",type:1,pageTitle:"Alternatives and missing features",url:"docs/alternatives-and-missing-features#missing-features",content:"QA-Board works with other tools. It won't replace or provide: Automation: we're not a CI tool, and won't provide logic needed to create build pipelines, or decide when to run what. In a CI system, QA-Board is just a standard executable, qa. If you're looking for a CI plaform, consider GitlabCI or DroneCI.Execution Environment: if you need containers, install the QA-Board CLI as a dependency, or implement the logic needed within your code. As of now, the only help QA-Board gives is support for .envrc files. Implementing more types of qa batch \"runners\" might fill the gap here.Monitoring & Deployement: in this space solutions tend to be custom of industry specific. Get in touch if you see low-hanging fruits!Data Versionning "},{title:"Planned features",type:1,pageTitle:"Alternatives and missing features",url:"docs/alternatives-and-missing-features#planned-features",content:"1 vs N comparaisons versus only pairs of versions.Pipelines / DAG, needed for calibration/training->evaluation pipelines. But stay tuned, we're working on that!More Viewers: flamgraphs, vega, links to open with notebooks, visdom, webiz...Users for login, and commenting on results. We also plan to introduct a number of smaller features "},{title:"Why not use X instead?",type:1,pageTitle:"Alternatives and missing features",url:"docs/alternatives-and-missing-features#why-not-use-x-instead",content:"Most comparable tools focus on training for machine learning (sacred, nni, mlflow, tensorboard, polyaxon...). Our use cases revolve around qualitative outputs for a wide range of algorithms. It means we need flexible visualizations. This said, those tools are great too! They often have features that qatools is still missing (labelling and commenting outputs, better GUI in some respects). Many of the commercial solutions (cometML, convrg.io, netpune.ai...) can provide a lot of value too, depending on your use-case and the size/maturity of your organization.Notebooks are amazing for experimentation and r&d reporting, but are not easy to compare and manage. tensorboard has a lot of qualities, but it doesn't scale to many experiments, doesn't know about git, and is not persistent. We may integrate an \"Open in Tensorboard\" button, ask about it and stay tuned. As for visdom, it's great for experimenting, less to store historical information. "},{title:"QA-Board's API",type:0,sectionRef:"#",url:"docs/apis",content:""},{title:"CLI API",type:1,pageTitle:"QA-Board's API",url:"docs/apis#cli-api",content:'If you run batches, it can be useful to know where results where saved. Copyqa batch my-batch \u2013-list-output-dirs##=> prints to STDOUT each output directory on a line#> $some/where/place#> $some/other/place#> ... qa batch my-batch \u2013-list##=> prints to STDOUT an array with info on each run# [# {# "absolute_input_path": "some/file"# "configurations": ["my-config"],# "output_directory": "some/place"# }# ] For specific needs, you can also use qa get to output internal variables: Copyqa --input my/input get output_directoryqa get commit_idqa get ci_root# etc '},{title:"HTTP API",type:1,pageTitle:"QA-Board's API",url:"docs/apis#http-api",content:'TODO: Formalize with an openapi/swagger spec. It\'s not documented yet, but quite stable and there are no surprises. Copycurl -k "https://qa/api/v1/commits?project=tof/swip_tof" | jq Copy[ // --snip-- { "id": "af9370b9246657e74e8e7fbd28c180b5cca7d3a7", "type": "git", "branch": "origin/HM1_develop", "parents": [ "c976adc9020683f90d51d25c39d9e177c621dc95" ], "message": "add dynamic min subtract\\n", "committer_name": "Rivka Emanuel", "committer_avatar_url": "http://gitlab-srv/uploads/-/system/user/avatar/164/avatar.png", "authored_datetime": "2019-04-30T09:05:09+00:00", "authored_date": "2019-04-30", "data": null, "commit_dir_url": "/s//stage/algo_data/ci/LSC/Calibration/commits/1556615109__Rivka Emanuel__af9370b9", "repo_commit_dir_url": "/s//stage/algo_data/ci/LSC/Calibration/commits/1556615109__Rivka Emanuel__af9370b9", "batches": { "default": { "id": 19172, "commit_id": "af9370b9246657e74e8e7fbd28c180b5cca7d3a7", "label": "default", "created_date": "2019-04-30T09:08:59.962664", "data": { "type": "ci" }, "output_dir_url": "/s/stage/algo_data/ci/LSC/Calibration/commits/1556615109__Rivka Emanuel__af9370b9/output", "aggregated_metrics": {}, "valid_outputs": 0, "pending_outputs": 0, "running_outputs": 0, "failed_outputs": 2 } }, "time_of_last_batch": "2019-04-30T09:05:09+00:00" }, // --snip--] Copycurl -k "$base_url/commit/01c27dfc4ffbf93ce95639b4dfbc126da4c53053?project=tof/swip_tof" | jq Copy{ "id": "2032a39564281de429e667260d1def3d16980e01", "type": "git", "branch": "origin/InvestigateCompressionGW1", "parents": [ "93b0a95d0ceaa78d24be33bea67a4aa333491c23" ], "message": "1. PARAMETERIZE compression\\n2. add gw1 package\\n", "committer_name": "Rivka Emanuel", "committer_avatar_url": "http://gitlab-srv/uploads/-/system/user/avatar/164/avatar.png", "authored_datetime": "2019-04-28T12:55:51+00:00", "authored_date": "2019-04-28", "data": { "qatools_config": { // --snip }, "qatools_metrics": { // --snip } } "commit_dir_url": "/s//stage/algo_data/ci/LSC/Calibration/commits/1556456151__Rivka Emanuel__2032a395", "repo_commit_dir_url": "/s//stage/algo_data/ci/LSC/Calibration/commits/1556456151__Rivka Emanuel__2032a395", "batches": { "default": { "id": 19071, "commit_id": "2032a39564281de429e667260d1def3d16980e01", "label": "default", "created_date": "2019-04-28T13:27:28.119816", "data": { "type": "ci" }, "output_dir_url": "/s/stage/algo_data/ci/LSC/Calibration/commits/1556456151__Rivka Emanuel__2032a395/output", "aggregated_metrics": {}, "valid_outputs": 1, "pending_outputs": 2, "running_outputs": 1, "failed_outputs": 2, "outputs": { "350582": { "id": 350582, "output_type": "", "platform": "windows", "configuration": "base_2X5", "extra_parameters": {}, "metrics": { "is_failed": false, "compute_time": 30.1366302967 "OTP_size_CrossTalk": 1040, "OTP_size_DayLight50": 464, // --snip }, "is_failed": false, "is_pending": false, "is_running": false, "data": { "ci": true }, "output_dir_url": "/s//stage/algo_data/ci/LSC/Calibration/commits/1556456151__Rivka Emanuel__2032a395/output/windows/base-2x5/Truly_LSC_DB/M01", "test_input_database": "\\\\\\\\netapp\\\\QA-Data\\\\2X5", "test_input_path": "Truly_LSC_DB\\\\M01", "test_input_tags": [] }, // --snip-- } } }, "time_of_last_batch": "2019-04-28T12:55:51+00:00"}  '},{title:"Auto-Optimization",type:0,sectionRef:"#",url:"docs/auto-optimization",content:""},{title:"qa optimize",type:1,pageTitle:"Auto-Optimization",url:"docs/auto-optimization#qa-optimize",content:'EXPERIMENTAL: This feature is experimental and the API is subject to change at any time.Specifically, we may change the solver backend to nevergrad, offer multiple choices, etc. You need the scikit-opt package, which you can install with Copypip install qatools[opt] Copyqa optimize --help WIP: We\'re working on section about "auto-optimization". Stay tuned! '},{title:"Getting SSL certificates from IT",type:0,sectionRef:"#",url:"docs/backend-admin/getting-ssl-certiticates-from-it",content:"Copycd deployment/nginx/ssl/qa # 1. Generate a key `.key`.openssl genrsa -out qa.key 2048 # 2. Generate a certificate request `.csr`.openssl req -new -sha256 -key qa.key -out qa.csr -config qa.csr.conf# Accept all the defaults:# - Country Name: IL# - State or Province Name: Israel# - Locality Name: Ramat Gan# - Organization Name: Samsung# - Organizational Unit Name: SIRC# - Common Name: *.qa# - Email: arthur.flam@samsung.com# - Password: (empty)# - Optionnal Company Name: (empty) # Check all is good.openssl req -noout -text -in qa.csr # 3. Send the CSR to IT.# 4. They will give you a `.cer` certificate. Convert it to `.pem` with openssl x509 -in dvs.cer -inform der -outform pem -out qa.pem # 5. Now you can configure your server to use qa.key and qa.pemReferences:nginx configurationmultiname certificates"},{title:"Upgrading the QA-Board host#Connect to the host",type:0,sectionRef:"#",url:"docs/backend-admin/host-upgrades",content:""},{title:"Check and save the latest daily backup",type:1,pageTitle:"Upgrading the QA-Board host#Connect to the host",url:"docs/backend-admin/host-upgrades#check-and-save-the-latest-daily-backup",content:"Copyls -lh /home/ispq/qaboard/database_backups/2020-01-07.dumpcp /home/ispq/qaboard/database_backups/2020-01-07.dump . "},{title:"To make the recovery easier",type:1,pageTitle:"Upgrading the QA-Board host#Connect to the host",url:"docs/backend-admin/host-upgrades#to-make-the-recovery-easier",content:"Copy# docker login gitlab-srv.transchip.com:4567docker push gitlab-srv.transchip.com:4567/common-infrastructure/qaboard:production "},{title:"Stop the server and create a backup",type:1,pageTitle:"Upgrading the QA-Board host#Connect to the host",url:"docs/backend-admin/host-upgrades#stop-the-server-and-create-a-backup",content:"Copydocker stop qaboard-production/home/arthurf/qaboard/backend/deployment/create-backup.sh "},{title:"MAINTENANCE",type:1,pageTitle:"Upgrading the QA-Board host#Connect to the host",url:"docs/backend-admin/host-upgrades#maintenance",content:"Storage/CPU upgrade... "},{title:"RECOVER",type:1,pageTitle:"Upgrading the QA-Board host#Connect to the host",url:"docs/backend-admin/host-upgrades#recover",content:"Check nginx is live Copysudo server nginx statussudo server nginx start # reload If not: check it's installedcheck it has the config under deployment/nginx Check the docker container is started If not: Copy# if no images...# docker pull gitlab-srv.transchip.com:4567/common-infrastructure/qaboard:productionCI_ENVIRONMENT_SLUG=production ~/qaboard/backend/deployment/start-docker.sh Check the database works. In case of issues, Recover from a backup: http://gitlab-srv/common-infrastructure/qaboard/tree/master/backend#recovery Copydocker restart qaboard-production #4. RESTART ssh planet31 #ABORT Check the docker container is started "},{title:"Troubleshooting common issues",type:0,sectionRef:"#",url:"docs/backend-admin/troubleshooting",content:""},{title:"How to get logs from QA-Board's backend",type:1,pageTitle:"Troubleshooting common issues",url:"docs/backend-admin/troubleshooting#how-to-get-logs-from-qa-boards-backend",content:"Copydocker logs -f --since 5m qaboard-production # is the container even running ? restarting all the timedocker ps # docker exec -it qaboard-production bash "},{title:"How to restart nginx if the server reboots",type:1,pageTitle:"Troubleshooting common issues",url:"docs/backend-admin/troubleshooting#how-to-restart-nginx-if-the-server-reboots",content:"Symptom: In case of timeouts when qa run/batch tries to contact the server (at http://qa:5000, http://dvs:5000). Copysudo service nginx start # status # restart "},{title:"Restart the docker container",type:1,pageTitle:"Troubleshooting common issues",url:"docs/backend-admin/troubleshooting#restart-the-docker-container",content:"Symptom: Issues accession pages at https://qa/500 errorsOften necessary of the disk gets full Copydocker restart qaboard-production ## and the image server... # qaboard_iiif_cantaloupe-production # qaboard_iip_cde-production "},{title:"Start from scratch the docker container",type:1,pageTitle:"Troubleshooting common issues",url:"docs/backend-admin/troubleshooting#start-from-scratch-the-docker-container",content:"Copydocker stop qaboard-productiondocker rm qaboard-production# ! it's arthurf's dev workspace, UNSTABLE# do your own clone...!CI_ENVIRONMENT_SLUG=production /home/arthurf/common-infrastructure/qaboard/backend/deployment/start-docker.sh "},{title:"What to do when the disk space is full",type:1,pageTitle:"Troubleshooting common issues",url:"docs/backend-admin/troubleshooting#what-to-do-when-the-disk-space-is-full",content:'Symptom: 500 errorsdatabase unreachable in the logs"no space left on device" in the logs Remove the image cache: Copydocker stop qaboard_iiif_cantaloupe-production && \\docker rm qaboard_iiif_cantaloupe-production && \\docker volume rm cache_cantaloupe && \\# restart the image serverdocker run --name qaboard_iiif_cantaloupe-production -p 8182:8182 -v cache_cantaloupe:/var/cache/cantaloupe -v /opt/dockermounts/stage/algo_data:/repository -v /srv/cantaloupe:/srv/cantaloupe --detach --restart always -it cantaloupe # you\'ll also likely need to restart the container Remove unused docker images Copydocker image prune '},{title:"Re-build and start the docker container",type:1,pageTitle:"Troubleshooting common issues",url:"docs/backend-admin/troubleshooting#re-build-and-start-the-docker-container",content:"Copycd webapp && \\# build the frontendnpm run build && \\# keep old JS bundles not to break users currently using the apprsync -r build deployed_build && \\cd .. && \\# re-build the containerdocker build -t gitlab-srv.transchip.com:4567/common-infrastructure/qaboard:production . && \\docker stop qaboard-production && \\docker rm qaboard-production && \\CI_ENVIRONMENT_SLUG=production ~/common-infrastructure/qaboard/backend/deployment/start-docker.sh "},{title:"Batches of inputs",type:0,sectionRef:"#",url:"docs/batches-running-on-multiple-inputs",content:""},{title:"Setting a custom database per batch",type:1,pageTitle:"Batches of inputs",url:"docs/batches-running-on-multiple-inputs#setting-a-custom-database-per-batch",content:"Copyyou-can-override-the-default-database: database: linux: /mnt/database windows: '\\\\\\\\storage\\\\database' inputs: - Images/Demo3/A.jpg - Images/Demo2 "},{title:"Specifying test configurations",type:1,pageTitle:"Batches of inputs",url:"docs/batches-running-on-multiple-inputs#specifying-test-configurations",content:'ReminderMake sure you read the section on configurations Let\'s look at examples from the HW_ALG project to illustrate how configurations can be given: Copyusing-a-custom-configuration: configurations: - base inputs: - Bayer/A.dng - Bayer/B.dng# => configurations == ["base"]# => the code would load base/config.cde Copymultiple-configurations: configurations: - base - low-light inputs: - Bayer/A.dng - Bayer/B.dng#=> configurations == ["base", "low-light"]#=> we merge 2 CDE configs  Copyconfigurations-can-be-complex-objects: configurations: - base - low-light - cde: - "-w 9920" - "-h 2448" - "-it BAYER10" inputs: - Bayer/A.dng - Bayer/B.dng# configurations == ["base", "low-light", {"cde": ["-w 9920", "-h 2448", "-it BAYER10"]}]# => Here we use the "cde" config parameter to pass CLI arguments to CDE. Copyeach-input-can-have-its-own-configuration: configurations: - base inputs: - Bayer/A.dng: #=> configurations == ["base"] - Bayer/B.dng: - low-light - cde: - "-DD" #=> configurations == ["base", "low-light", {"cde": ["-DD"]}] '},{title:"Organizing your groups of inputs",type:1,pageTitle:"Batches of inputs",url:"docs/batches-running-on-multiple-inputs#organizing-your-groups-of-inputs",content:""},{title:"Group aliases",type:1,pageTitle:"Batches of inputs",url:"docs/batches-running-on-multiple-inputs#group-aliases",content:"For convenience you can define aliases for groups you often run together. For instance you can do: Copy# qa/batches.yamlgroups: two-batches: - first-batch - second-batch Copyqa batch two-batches "},{title:"Configuration aliases",type:1,pageTitle:"Batches of inputs",url:"docs/batches-running-on-multiple-inputs#configuration-aliases",content:'For convenience you can define YAML aliases for common configurations Copy.base: &base - base - partial hdr: configurations: - *base - hdr inputs: - A - B - C#=> configurations == ["base", "partial", "hdr"] noteYAML "aliases" and "anchors" are standard YAML feature. Read more here. '},{title:"Reusable configurations/inputs",type:1,pageTitle:"Batches of inputs",url:"docs/batches-running-on-multiple-inputs#reusable-configurationsinputs",content:"Sometimes you want to mix and match reusabe definitions of configs and inputs. YAML anchors let you do it: Copy.inputs_hdr: &inputs_hdr inputs: - A - B .lots_of_inputs_hdr: &lots_inputs_hdr inputs: - A - B - C - D - E - F .HDR: &HDR configurations: - *base - hdr_base - hdr_motion .HDR-disabled: &HDR-disabled configurations: - *base hdr: <<: *HDR <<: *inputs_hdrno-hdr: <<: *HDR-disabled <<: *inputs_hdr# qa --batch-label hdr batch hdr# qa --batch-label no-hdr batch no-hdr # Maybe on nightly runs you want to run lots of inputslots-of-hdr-inputs: <<: *HDR <<: *lots_inputs_hdrlots-of-no-hdr-inputs: <<: *HDR-disabled <<: *lots_inputs_hdr "},{title:"Bit accuracy tests",type:0,sectionRef:"#",url:"docs/bit-accuracy",content:""},{title:'"Soft" bit-accuracy checks from the UI',type:1,pageTitle:"Bit accuracy tests",url:"docs/bit-accuracy#soft-bit-accuracy-checks-from-the-ui",content:"The web application lets you view and compare all files created by your algorithm's runs: Files are marked depending on their status (identical, different, added, removed...). Identical files are hidden by default.You can click on each file to open it with an appropriate file viewer:  noteThe UI doesn't care about qaboard.yaml's bit-accuracy.patterns (discussed later). "},{title:"Sample CI for bit-accuracy checks",type:1,pageTitle:"Bit accuracy tests",url:"docs/bit-accuracy#sample-ci-for-bit-accuracy-checks",content:"You often want to know when your algorithm's results change, especially if another team is busy implementing them in hardware! Here is how you could get the CI to warn you with GitlabCI: Copystages: - tests - bit-accuracy tests-all: stage: tests script: - qa batch all bit-accuracy-all: stage: bit-accuracy allowed_failure: true script: - qa check-bit-accuracy --batch all "},{title:'"Hard" qa check-bit-accuracy on the CLI',type:1,pageTitle:"Bit accuracy tests",url:"docs/bit-accuracy#hard-qa-check-bit-accuracy-on-the-cli",content:"qa check-bit-accuracy $batch compares the results of qa batch $batch to: The latest results on project.reference_branch from qaboard.yaml (default: master)....unlesss you're checking a merge made to that branch. In which case the commit's parents will act as references.You can ask to compare versus a specific git commit, branch or tag with qa check-bit-accuracy --reference $git-ref. If the commit you compare against has not finished its CI, qa will wait. Custom NeedsYou can opt-in to more complex behaviour in qaboard.yaml with bit-accuracy.on_reference_failed_ci, in case there are not results in the reference commit. Maybe the build failed, in which case you want to compare against the previous commit... If you're interested open an issue we'll add more details to the docs. If output files are different, qa prints a report and exists with a failure. noteFor specific use-case, there is also qa check-bit-accuracy-manifest which checks accuracy versus a manifest file stored in the database next to the input files. To generate those manifests, use qa batch --save-manifests-in-database. "},{title:"What files are checked?",type:1,pageTitle:"Bit accuracy tests",url:"docs/bit-accuracy#what-files-are-checked",content:"Files matching the patterns defined as bit-accuracy.patterns in qaboard.yaml will be checked. noteIf you work with text files on both Linux and Windows, EOL can make things tricky... You can decide what files are plaintext or binary using bit-accuracy.plaintext or bit-accuracy.binary. "},{title:"Using celery as a task runner",type:0,sectionRef:"#",url:"docs/celery-integration",content:""},{title:"Starting Celery workers",type:1,pageTitle:"Using celery as a task runner",url:"docs/celery-integration#starting-celery-workers",content:"To manage the task queue we'll need what they call a broker. It's easy to start one: Copydocker run --detach -p 5672:5672 rabbitmq#=> runs in the backgroud, stays alive Next you need to start at least a worker that will execute async tasks: Copypip install celerycelery -A qaboard.runners.celery_app worker --loglevel=info Next, use QA-Board's Celery runner: Copyqa batch --runner=celery my-batch "},{title:"Configuring Celery",type:1,pageTitle:"Using celery as a task runner",url:"docs/celery-integration#configuring-celery",content:"To configure Celery at the project level: Copy# qaboard.yamlrunners: default: celery celery: # Those are the default settings: broker_url: pyamqp://guest@localhost// # also read from ENV vars with CELERY_BROKER_URL result_backend: rpc:// # also read from ENV vars with CELERY_RESULT_BACKEND # To know all the options and tweak priorities, rate-limiting... Read: # http://docs.celeryproject.org/en/latest/getting-started/first-steps-with-celery.html#configuration # http://docs.celeryproject.org/en/latest/userguide/configuration.html#configuration # For example: timezone: Europe/Paris # By default tasks will be named \"qaboard\" unless you define qaboard_task_name: qaboard It's often useful to give batches their own settings. For instance you may want to use different queues if you manage different types of resources (GPUs, Windows/Linux/Android...): Copy# On a server with a GPU:celery -A qaboard.runners.celery_app worker --concurrency=1 --queues gpu,large-gpu Copy# qa/batches.yamlmy-batch-that-needs-a-gpu: inputs: - my/training/images configuration: - hyperparams.yaml celery: task_routes: qaboard: gpu tipRead Celery's tutorial Celery's worker user guide has lots of information on how to run worker in the background, set concurrency... Check it out too as needed! If you need worker monitoring, read the docs. "},{title:"Integrating qatools with your CI",type:0,sectionRef:"#",url:"docs/ci-integration",content:""},{title:"Requirement",type:1,pageTitle:"Integrating qatools with your CI",url:"docs/ci-integration#requirement",content:"Make sure your Gitlab project has an integration with qatools. If you're not sure if/how, review the qatools setup guide. You should be able to see your project in the qatools web application:.  "},{title:"Running qatools in your CI",type:1,pageTitle:"Integrating qatools with your CI",url:"docs/ci-integration#running-qatools-in-your-ci",content:"Have your CI launch qatools: With GitlabCI, you would do something like: Copy# gitlab-ci.ymlqa-tests: stage: test script: # assuming you defined a batch named ci - qa batch ci noteYou CI is responsible for setting up an environment ($PATH...) in which qatools is installed! Consider using docker, or sourcing a configuration file... Push a commit to Gitlab. If your CI is successful, the commit will appear in your project's page:   "},{title:"Example with GitlabCI",type:1,pageTitle:"Integrating qatools with your CI",url:"docs/ci-integration#example-with-gitlabci",content:"qatools knows how to work with the most common CI tools: GitlabCI, Jenkins... Copy# .gitlab-ci.yamlstages: - build - qa build-linux: stage: build script: - make - qa save-artifacts qa-tests stage: qa script: - qa batch ci "},{title:"Optionnal CI helpers",type:1,pageTitle:"Integrating qatools with your CI",url:"docs/ci-integration#optionnal-ci-helpers",content:'qatools is not a CI tool, but it provide some utilities to run code only in some branches: cautionThis logic is usually better expressed in your CI tool itself. Copy# ci.pyfrom qatools.ci_helpers import on_branch, run_tests @on_branch(\'develop\')def my_tests(): pass # Also supported:# @on_branch(["develop", "master"])# @on_branch("feature/*") if __name__ == \'__main__\': run_tests() Copypython ci.py '},{title:"Computing quantitative metrics",type:0,sectionRef:"#",url:"docs/computing-quantitative-metrics",content:"Algorithms are usually evaluated using KPIs / Objective Figures of Merit / metrics. To make sure qatools's web UI displays them:run() should return a dict of metrics:Copydef run(): # --snip-- return { \"loss\": loss }Describe your metrics in qa/metrics.yaml. Here is an exampleCopy# qa/metrics.yaml (location from qaboard.yaml: outputs.metrics)available_metrics: loss: # the fields below are all optionnal label: Loss function # human-readable name short_label: Loss # somes part of the UI are better with thin labels... smaller_is_better: true # default: true target: 0.01 # plots in the UI will compare KPIs versus a target if given # when displaying results in the UI, you often want to change units # scale: 100 # suffix: '%'If it all goes well you get:Tables to compare KPIs per-input across versions:Summaries:Metrics integrated in the visualizations:and evolution over time per branch...noteWe plan on not requiring you to define metrics ahead of time."},{title:"Creating and viewing outputs files",type:0,sectionRef:"#",url:"docs/creating-and-viewing-outputs-files",content:""},{title:"Accessing output files",type:1,pageTitle:"Creating and viewing outputs files",url:"docs/creating-and-viewing-outputs-files#accessing-output-files",content:"All the outputs are saved as files. To get them out and qatools provides multiple ways to get them out. Next to each output, there is always a button to copy-to-clipboard the path to the files it created.  From the Navigation bar, you can copy-to-clipboard the windows-ish path where each commit saves its results: "},{title:"Defining Pipelines / DAG",type:0,sectionRef:"#",url:"docs/dag-pipelines",content:'Work-In-ProgressNothing here is implemented currently, it\'s at the API design stage.Currently QA-Board lacks expressivity for the common use-case of: 1. Run CDE on some images 2. Calibration 3. ValidationLikewise, it can\u2019t handle nicely machine learning workflows (training/validation).Below are two workarounds people have used until now, and a proposition for built-in support in QA-Board.Can you send me feedback / alternative ideas, or share to relevant people? Especially if you have experience with various flow engines, e.g. DVC. Thanks!The goal is a solution that is simple, expressive, and enables caching.Run qa batch multiple times, with each run expecting that results of the previous run are available (as done with @rivka, @ToF). a. (+) Rather easy to do b. (-) The logic is outside QA-Board \u2013 it can\u2019t easily be used for tuning from the web UI \uf04bCreate a "meta" run, with a heavy run() function that itself takes care of everything (as done with @eliav) a. (+) Easy to do b. (-) But very custom and not easy to use c. (-) There many tricky corners (running locally, filesystem issues on LSF) that should not be the engineer\u2019s concern."Built-in support" by QA-Board. One possible way we could do it is by extending the syntax used to defined batches with a depends:/ needs: keyword:Simple example:Copybatch1: inputs: - A.jpg - B.jpg configurations: - base batch2: depends: batch1 type: script configurations: - python my_script.py {o.output_dir for o in depends["batch1"]} # ? there is not really an "input" for a script, especially if it uses "depends"# ? if we depend on something I guess we could do without input, provide None..."Real-world" example:Copymy-calibration-images: configurations: - base inputs: - DL50.raw - DL55.raw - DL65.raw - DL75.raw my-calibration: depends: calibration_images: my-calibration-images type: script configurations: - python calibration.py ${o.output_directory for o in depends[calibration_images]} my-evaluation-batch: depends: calibration: my-calibration inputs: - test_image_1.raw - test_image_2.raw - test_image_3.raw configurations: - base - ${depends[calibration].output_directory}/calibration.cdeCopy$ qa batch my-evaluation-batch#=> qa batch my-calibration-images#=> qa batch my-calibration#=> qa batch my-evaluation-batch(+) simple for users (I think so?)(+) caching for free(?) need to define a clear API: how each job can lookup results of earlier jobs\u2026 And likely we will need naming-conventions for parameter tuning\u2026'},{title:"Debugging QA-Board' runs in an IDE",type:0,sectionRef:"#",url:"docs/debugging-runs-with-an-IDE",content:""},{title:"Debugging with PyCharm",type:1,pageTitle:"Debugging QA-Board' runs in an IDE",url:"docs/debugging-runs-with-an-IDE#debugging-with-pycharm",content:'Edit your "debug configurations" like this: Module name: qatools (make sure you select "module" not "script" in the dropdown menu).Parameters: CLI parameters for qa: run -i images/A.jpg.Working directory: Check it\u2019s defined as the directory with qaboard.yaml. If this directory happens to have a subfolder named "qatools", use it.  In some cases you\'ll also need to define as environment variables LC_ALL=en_US.utf8 LANG=en_US.utf8 '},{title:"Debugging with VSCode",type:1,pageTitle:"Debugging QA-Board' runs in an IDE",url:"docs/debugging-runs-with-an-IDE#debugging-with-vscode",content:'To configure debugging, the editor opens a file called launch.json. You want to add configurations that look like those: Copy{ "name": "qatools", "type": "python", "request": "launch", "module": "qatools", "args": [ "--", // needed... "--help", ]}, Copy{ "--", "--inputs-database", ".", "run", "--input-path", "tv/tv_GW1_9296x256_REMOSAIC_V1_FULL_X_HP_PDA1",} Here is a more in-depth review of your options at https://code.visualstudio.com/docs/python/debugging '},{title:"Deleting Old Outputs and Artifacts",type:0,sectionRef:"#",url:"docs/deleting-old-data",content:""},{title:"What data will not be deleted",type:1,pageTitle:"Deleting Old Outputs and Artifacts",url:"docs/deleting-old-data#what-data-will-not-be-deleted",content:"Outputs from commits that are either: Recent (more info below)On the project.reference_branch from qaboard.yaml.Are on a commit/tag/branch listed as a project.milestones in qaboard.yaml.Are a milestone defined from QA-Board's UI. cautionQA-Board will set as a commit's branch the first it was seen on. If you merge with fast-forward rebased branches, then this information will not be what you expect.  "},{title:"Configuring garbage collection",type:1,pageTitle:"Deleting Old Outputs and Artifacts",url:"docs/deleting-old-data#configuring-garbage-collection",content:"Data can be erased after a period of time where the commit has no new outputs. Copy# qaboard.yamlstorage: garbage: after: 1month after: supports human-readable values like 2weeks, 1year, 3months... "},{title:"Recovering lost data?",type:1,pageTitle:"Deleting Old Outputs and Artifacts",url:"docs/deleting-old-data#recovering-lost-data",content:"Well, you won't be able to do that. What you should try to do is make everything reproducable. Define your whole environment as code. Make sure your commits contain 100% of what is needed for your code to run. Tools you can use include docker+Dockerfile, etc.Make it easy to re-trigger your CI, so that it's straightfoward to re-builds, re-run your tests, and uploads artifacts to QA-Board.If necessary, make it also very easy to run manually something like Copygit checkout $hexshamakeqa save-artifactsqa batch my-batch "},{title:"Deleting commit artifacts",type:1,pageTitle:"Deleting Old Outputs and Artifacts",url:"docs/deleting-old-data#deleting-commit-artifacts",content:"Artifacts are not deleted by default, you have to ask for it: Copystorage: garbage: after: 1month artifacts: delete: true If you want to keep some artifacts (maybe \"small\" coverage reports defined as coverage_report: ... in qaboard.yaml's artifacts) Copystorage: garbage: after: 1month artifacts: delete: true keep: - coverage_report # also supports relative artifacts paths, e.g. - build/my_binary Notes: The settings that are used are those in the latest commit of the reference_branch defined in qaboard.yaml If you change those settings, artifacts for already deleted commits don't get deleted.When a qa run uses a commit that was deleted, or if you upload manifests for a deleted commit, it is marked as undeleted. "},{title:"Frequently Asked Questions",type:0,sectionRef:"#",url:"docs/faq",content:""},{title:"There is a bug I'd like you to know about",type:1,pageTitle:"Frequently Asked Questions",url:"docs/faq#there-is-a-bug-id-like-you-to-know-about",content:"Arthur Flam "},{title:"What is QA-Board written with?",type:1,pageTitle:"Frequently Asked Questions",url:"docs/faq#what-is-qa-board-written-with",content:"CLI tool (wraps your code): pythonFrontend: views with reactjs, state with reduxjs, design with blueprintjs, images with openseadragon, plots with plotly/threejs...Backend: postgreSQL (to store metadata) accessed via flask "},{title:"Does QA-Board work with python2.7?",type:1,pageTitle:"Frequently Asked Questions",url:"docs/faq#does-qa-board-work-with-python27",content:"Well enough! Just call python2 your_code.py as any other executable. "},{title:"Where are results saved?",type:1,pageTitle:"Frequently Asked Questions",url:"docs/faq#where-are-results-saved",content:"Local runs are saved under the output/ directory in the project.During CI runs, results are saved under the ci_root defined in qaboard.yaml. To be honest, the exact naming conventions is complicated... Export the data using the UI's export utilities, or ask qatools' simple API.  "},{title:"Can I export the data or use a third-party viewer?",type:1,pageTitle:"Frequently Asked Questions",url:"docs/faq#can-i-export-the-data-or-use-a-third-party-viewer",content:'Yes! All the outputs are saved as files, and qatools provides multiple ways to get them out. cautionAt the moment nothing prevents your from modifying/destroying files created from the CI. In the "Visualization" tab, an export utility lets you copy-to-clipboard a path with filtered/nicely-renamed results/files: Next to each output, there is always a button to copy-to-clipboard the path to the files it created.  From the Navigation bar, you can copy-to-clipboard the windows-ish path where each commit saves its results:You can also programmatically access qatools\'s data by querying its API. '},{title:"Input files",type:0,sectionRef:"#",url:"docs/inputs",content:""},{title:"Batches of inputs",type:1,pageTitle:"Input files",url:"docs/inputs#batches-of-inputs",content:"To run on batches of multiple inputs, use qa batch my-batch, where my-batch is defined in: Copy# qa/batches.yaml (can be changed in qaboard.yaml via inputs.batches)my-batch: inputs: - images/A.jpg - images/B.jpg Copyqa batch my-batch#=> qa run --input images/A.jpg#=> qa run --input images/B.jpg noteWe'll cover batches in more depth later. By default, batches run in parallel locally, but you can easily setup an async task queue like Celery or others. "},{title:"Identifying inputs (Recommended)",type:1,pageTitle:"Input files",url:"docs/inputs#identifying-inputs-recommended",content:"You'll often want to do something like \"run on all the images in a given folder\". For that to work, you have to tell QA-Board how to identify your images as inputs. In qaboard.yaml edit and inputs.globs with a glob pattern. Here is an example where your inputs are .jpg images: Copyinputs: globs: '*.jpg' When you do this, you no longer have to define an explicit list of input paths in your batches. You can instead use folders or even globs/wildcards (*, **...): Copy# qa/batches.yamlmy-batch: inputs: - images tipTo run on all the inputs found under $database / $PATH you can simply use qa batch $PATH. You can give multiple patterns: Copyinputs: globs: - '*.jpg' - '*.bmp' - '*.dng' A common use case is identifying folders containing a file patching a pattern, for instance movies given a sequence of frames, frame_000.jpg, frame_001.jpg... In this case you can use use_parent_folder: Copyinputs: globs: frame_000.jpg use_parent_folder: false "},{title:"Handling multiple input types (Advanced)",type:1,pageTitle:"Input files",url:"docs/inputs#handling-multiple-input-types-advanced",content:"Big projects sometimes need to distinguish different types of inputs, which will be processed with a different logic. Copy# qaboard.yamlinputs: types: default: image image: globs: '*.raw' movie: globs: frame_000.jpg use_parent_folder: true # you can override the defaults per-type database: linux: /mnt/movies windows: F://movies You can choose what type each batch is:  Copy# qa/batches.yamlmy-images: inputs: - my/image.jpg my-batch: type: movie inputs: - folder/of/movies - other/movies If needed, you can also specify the input type from the CLI: Copyqa batch my-imagess # by default look for imagesqa --type movie batch my-movies # here we look for movies "},{title:"Installing QA-Board's CLI",type:0,sectionRef:"#",url:"docs/installation",content:"To use QA-Board you need it pip install the qaboard package. On both Linux and Windows:Copypip install --upgrade git+ssh://git@gitlab-srv/common-infrastructure/qatools# If you have SSL/certificates/trust errors, use --trusted-host pypi.python.org --trusted-host pypi.org --trusted-host files.pythonhosted.org# If you have timeouts, not authorized, proxy errors, or \"this is not a git repo error\", use --proxy http://dlp2-wcg01:8080 # in case of proxy issues (TIMEOUT error) in case of SSL/proxy issues # If you don't have pip / permissions, it means your python environment sucks.# If you're not a python pro, simply install python with the anaconda distribution.# https://www.anaconda.com/distribution/#download-sectionTo make sure the installation was successful, try printing a list of qatools' CLI commands:Copyqa --help"},{title:"QA-Board is a platform for algorithms R&D",type:0,sectionRef:"#",url:"docs/introduction",content:""},{title:"Features",type:1,pageTitle:"QA-Board is a platform for algorithms R&D",url:"docs/introduction#features",content:"Organize, View and Compare Results, Tuning/OptimizationWeb-based: sharable URLs, no install needed.Visualizations: support for quantitative metrics, and many file formats: advanced image viewer, support for videos, plotly graphs, text, pointclouds, embedded HTML...Integrations: direct access from Git/CI, easily exportable results, API, links to the code, trigger jobs...Samsung colleagues can ask for and get access! "},{title:"Achieved Benefits",type:1,pageTitle:"QA-Board is a platform for algorithms R&D",url:"docs/introduction#achieved-benefits",content:"Scale R&D: enable engineers to achieve more and be more productive.Faster Time-to-Market: collaboration across teams, workflow integration..Quality: uncover issues earlier, KPIs, tuning, reporting... "},{title:"LSF Integration",type:0,sectionRef:"#",url:"docs/lsf-integration",content:""},{title:"LSF project options",type:1,pageTitle:"LSF Integration",url:"docs/lsf-integration#lsf-project-options",content:"You can change the default LSF configuration with: Copy# qaboard.yamlrunners: # In doubt, ask advice from your manager / CAD / bqueues. lsf: user: username queue: your_queue # qatools uses a fast queue to launch jobs that create subsequent LSF jobs # It helps get faster feeback about which outputs are pending # fast_queue: your_queue # threads: 0 # ask for eg 8 max threads when sending jobs to LSF (0=default) # memory: 0 # ask for eg 8000M memory when sending jobs to LSF (0=default) warningqa doesn't use LSF's job arrays. If your algorithm takes very little time to run, maybe using them would be better. Create an issue or contact Arthur Flam.  "},{title:"LSF options per batch",type:1,pageTitle:"LSF Integration",url:"docs/lsf-integration#lsf-options-per-batch",content:"Copy# qa/batches.yamlyou-can-give-an-LSF-configuration: lsf: memory: 1000 threads: 1000 configurations: - base inputs: - images/A.jpg - images/B.jpg Copyyou-can-give-an-LSF-configuration-per-input: lsf: memory: 1000 configuration: - base inputs: images/A.jpg: images/B.jpg: lsf: memory: 200 "},{title:"LSF options on the CLI",type:1,pageTitle:"LSF Integration",url:"docs/lsf-integration#lsf-options-on-the-cli",content:"You can use CLI options to override the defaults: Copyqa batch --help# --snip-- --lsf-threads INTEGER restrict number of lsf threads to use. 0=no restriction --lsf-memory INTEGER restrict memory (MB) to use. 0=no restriction --lsf-resources TEXT LSF resources restrictions (-R) --lsf-sequential / --lsf-parallel "},{title:'Connecting to LSF via a "bridge" host',type:1,pageTitle:"LSF Integration",url:"docs/lsf-integration#connecting-to-lsf-via-a-bridge-host",content:"It's needed for the server, that runs in a container, and sometimes to send LSF jobs from windows. There are two options: Via environment variables: Copyexport QA_RUNNERS_LSF_BRIDGE='ssh my_host_with_lsf_access {bsub_command}'export QA_RUNNERS_LSF_BRIDGE='ssh my_host_with_lsf_access su {user} {bsub_command}' Via project configuraton: Copy# qaboard.yamlrunners: lsf: # --snip-- bridge: 'ssh bridge_host' tipWhen using bridges, qa will explicitely ask LSF to run in your current working directory, so no need to play games with ssh 'cd {cwd} && {bsub_command}'... "},{title:"Using input metadata and integrating with external databases",type:0,sectionRef:"#",url:"docs/metadata-integration-external-databases",content:""},{title:"Metadata",type:1,pageTitle:"Using input metadata and integrating with external databases",url:"docs/metadata-integration-external-databases#metadata",content:"Input metadata are useful to: Filter and group inputsDecide what metrics to compute on your outputs To enable metadata support in qatools, implement in your project's entrypoint a function that returns metadata as a dict. Here is an example: Copy# qa/main.py (qaboard.yaml: project.entrypoint)def metadata(absolute_input_path, database, input_path): metadata_file = absolute_input_path.with_suffix('.metadata.yaml') if not metadata_file.exists(): return {} with metadata_file.open() as f: return yaml.load(f, Loader=yaml.SafeLoader) tipIf you define metadata.label it will be used in the UI instead of the input path. tipQA-Board will compares runs with different input if they have the same metadata.id. A common use-case is comparing images from different sensors taken in the same conditions. Qatools will forward metadata to your run() function as ctx.obj['input_metadata']. "},{title:"Integrating with external input databases",type:1,pageTitle:"Using input metadata and integrating with external databases",url:"docs/metadata-integration-external-databases#integrating-with-external-input-databases",content:'Instead of relying on walking on the filesystem, you can use an external database to organize your inputs. To enable this with qatools, implement in your project\'s entrypoint a function that iterates over inputs given a query: Copy# qa/main.pydef iter_inputs(path, database, only, exclude, inputs_settings): # TODO: Maybe here connect to an SQL database # and execute something like # f"SELECT test, metadata from tests where path LIKE {path} and database={database}" # OPTIONALLY: return filtered inputs using only/exclude # even if you don\'t do it, qatools will always re-filter # but doing it yourself in SQL can be much more efficient return ({"absolute_input_path": database / p.path, "metadata": p.metadata} for p in inputs) # Note: path=None should match all inputs in the database# Note: inputs_settings is a dict with information on how inputs should be found: file globs, use_parent, or anything else you put in qaboard.yaml\'s inputs. noteCurrently, you still have to write a metadata() function for run() to receive the metadata or for qatools to use them in the UI. '},{title:"Using metadata to filter batches of inputs",type:1,pageTitle:"Using input metadata and integrating with external databases",url:"docs/metadata-integration-external-databases#using-metadata-to-filter-batches-of-inputs",content:"Copyinputs-filtered-using-metadata: only: # only run tests matching all those conditions PD pattern: 4PD Model # mulitple options are OK - 2T7 - XXX Binning: '1:*' # wildcards are supported Bad pixels: False # as well as Booleans, numbers\u2026 Distance: '>1' # also >=, =,==, <, <= exclude: # don't run on tests matching all the filters below Location: Outdoor Copyqa batch inputs-filtered-using-metadata# => run only on inputs with 4PD as PD pattern, etc. "},{title:"metrics-quantitative-outputs",type:0,sectionRef:"#",url:"docs/metrics-quantitative-outputs",content:"metrics-quantitative-outputs.md"},{title:"Monorepo and subproject support",type:0,sectionRef:"#",url:"docs/monorepo-support",content:""},{title:"",type:1,pageTitle:"Monorepo and subproject support",url:"docs/monorepo-support#undefined",content:"Paths in qaboard.yaml are always relative to the repository root.project.avatar_url and project.descriptionfrom qatools import ci_repo_root.... "},{title:"qatools inheritance",type:1,pageTitle:"Monorepo and subproject support",url:"docs/monorepo-support#qatools-inheritance",content:"inheritsuper "},{title:"Various Tuning Workflows",type:0,sectionRef:"#",url:"docs/processing-batches-with-async-task-queues",content:""},{title:"Adding QA-Board to your project",type:0,sectionRef:"#",url:"docs/project-init",content:""},{title:"Gitlab Integration",type:1,pageTitle:"Adding QA-Board to your project",url:"docs/project-init#gitlab-integration",content:"Create a Gitlab integration to keep the QA-Board and git in sync. Be one of the project's Masters / Maintainers.Go to http://gitlab-srv/$YOUR_GROUP/PROJECT/settings/integrations.Add an integration with: URL: http://qa:5000/webhook/gitlabSecret token: (leave the field empty) To test everything went well, Gitlab lets you \"Test\" your new hook. You should get a blue happy 200 OK message \ud83d\udd35\ud83c\udf89. To make sure you can view your runs...Commit thoses changes and push!For now, the web interface can only show runs from commit that were pushed to Gitlab. We plan on removing this restriction and even the need to setup an integration. We'll also support other git servers (e.g. GitHub). "},{title:"References & Milestones",type:0,sectionRef:"#",url:"docs/references-and-milestones",content:""},{title:"Comparing versus a reference",type:1,pageTitle:"References & Milestones",url:"docs/references-and-milestones#comparing-versus-a-reference",content:"In results pages, QA-Board always compares the commit you selected (labeled new) versus a reference (ref):  The reference is by default the latest commit from the project's reference branch: Copy# qaboard.yamlproject: reference_branch: master To change the selected new or ref commit, you can edit the commit ID field in the navbar. Hovering it gives you a menu with other options:  tipClicking on the branch name in the navbar will select the latest commit on the branch. "},{title:"Project References",type:1,pageTitle:"References & Milestones",url:"docs/references-and-milestones#project-references",content:"You can also list in qaboard.yaml other versions as milestones. Copy# qaboard.yamlproject: reference_branch: master milestones: - release/v1.0.0 # tag - feature/better-perf # branch - e45123a3565 # commit id "},{title:"Defining Milestones from QA-Board",type:1,pageTitle:"References & Milestones",url:"docs/references-and-milestones#defining-milestones-from-qa-board",content:"Every user can save milestones with the \u201cstar\u201d icon in each commit navbar:  If needed, you can give them a name and leave notes:  You'll now be able to select them in the commit ID hover menu. tipMilestones can be shared with everybody - or kept private. "},{title:"Running your code",type:0,sectionRef:"#",url:"docs/running-your-code",content:""},{title:"Wrapping your code",type:1,pageTitle:"Running your code",url:"docs/running-your-code#wrapping-your-code",content:"How does it work? When you pip install QA-Board with pip, you get the qa executable. qa opens qaboard.yaml and imports the python file specified by project.entrypoint. Then it runs your entrypoint's run() function with information about the current run: input, configuration, where outputs should be saved etc. Take a look at the default run() in qatools/main.py. You should change it to run your code. In most cases that means finding and executing an exectuable file, or importing+running python code... tipMany users want to separate algorithm runs and postprocessing. To make this flow easier, you can optionnaly implement postprocess(). Then you will get qa run and qa postprocess. "},{title:"What should your wrapper do?",type:1,pageTitle:"Running your code",url:"docs/running-your-code#what-should-your-wrapper-do",content:'The main assumption is that your code Write "qualitative" files in the output_directoryReturns "quantitative" metrics/KPIs. The run() function receives as argument a context object whose properties tell us how you should run what, and where outputs are expected to be saved. importantBelow are the most common ways users wrap their code. Identify what works for you and continue to the next page!  '},{title:"Useful context properties (Reference)",type:1,pageTitle:"Running your code",url:"docs/running-your-code#useful-context-properties-reference",content:"noteYes, the API is ugly, it will change before the open-source release and we're open to suggestions!  What absolute_input_path $database / $input_path database path to the database input_path path of the test, relative to the database input_type input type input_metadata if relevant, input metadata (more info later)   How configurations array of strings or dicts. You decide how to interpret it! extra_parameters When doing tuning, a dict of key:values that should override specific algo parameters. platform Usually the host (linux/windows), but can be overwritten as part of your custom logic forwarded_args Extra CLI flags provided to qa. Usually used for debugging.   Where output_directory where your code should save its outputs  "},{title:"Accessing the QA-Board configuration from the entrypoint (Reference)",type:1,pageTitle:"Running your code",url:"docs/running-your-code#accessing-the-qa-board-configuration-from-the-entrypoint-reference",content:"Copyfrom qaboard.config import configconfig['project']['name']## etc Work in ProgressA full reference for from qaboard.config import ... will arrive in the docs! "},{title:"Use-case #1: Running Python code",type:1,pageTitle:"Running your code",url:"docs/running-your-code#use-case-1-running-python-code",content:"Copydef run(context): # Assuming your code is at *src/my_code.py* while the entrypoint is at *qa/main.py* sys.path.append(str(Path(__file__).parent.parent)) from src.my_code import MyRun # Note: you could also pip install your code as a package... # People commonly wrap their code within Classes/functions metrics = MyRun( input=context.obj['absolute_input_path'], output=context.obj['output_directory'], # The next page will show you to supply configurations params={\"hard-coded\": \"values\"}, ).run() metrics['is_failed'] = False # True if your code raises an exception return metrics "},{title:"Use-case #2: Running an executable",type:1,pageTitle:"Running your code",url:"docs/running-your-code#use-case-2-running-an-executable",content:'QA-Board assumes you already built your code.  Copyimport osimport sysfrom pathlib import Path def binary_path(): """Find and return the path of the executable. It\'s often different on Windows/Linux...""" if os.environ.get(\'PROJECT_BINARY\'): # Overwrite location via ENV variables return Path(os.environ[\'PROJECT_BINARY\']) if sys.platform == \'win32\': return Path("build/x64/my_binary.exe") else: # Easy support for build types: Release/Debug/Coverage/ASAN... return Path(f"build/{os.environ.get(\'PROJECT_BUILD_TYPE\')}/my_binary") def run(): command = [ f\'{binary_path()}\', "--input", str(context.obj["absolute_input_path"]), "--output", str(context.obj["output_directory"]), *context.obj["output_directory"] ] process = subprocess.run( command, check=True, # raise exception on exit code != 0 capture_output=True, # >=python3.7 ) # Note: If you want live interleaved STDOUT/STDERR, (like sample entrypoint from `qa init`), # it get a bit more complicated: https://docs.python.org/3/library/subprocess.html print(process.stdout) print(process.stderr) return {"is_failed": False} tipInstead of returning metrics, if you don\'t want to touch too much python, you can simply write them as JSON in $output_directory/metrics.json. '},{title:"Use-case #3: Importing existing results (Advanced)",type:1,pageTitle:"Running your code",url:"docs/running-your-code#use-case-3-importing-existing-results-advanced",content:"It's is sometimes needed to easily compare results versus reference implementations or benchmarks. Let's say the benchmark results can be found alongside images in your database like so: Copydatabase\u251c\u2500\u2500 images\u2502 \u251c\u2500\u2500 A.jpg\u2502 \u2514\u2500\u2500 B.jpg\u2514\u2500\u2500 standard-benchmark \u2514\u2500\u2500 images \u251c\u2500\u2500 A \u2502 \u2514\u2500\u2500 output.jpg \u2514\u2500\u2500 B \u2514\u2500\u2500 output.jpg Copydef run(context): if context.obj[\"input_type\"] == 'benchmark': import shutil # Next page you will learn how you can provided configurations/parameters to the run. benchmark = context.obj['configurations'][0]['benchmark'] # Find the benchmark results... benchmark_outputs = context.obj['database'] / benchmark context.obj['input_path'].parent / context.obj['input_path'].stem # To copy the result image only os.copy(str(benchmark_outputs / 'output.jpg'), str(context.obj['output_directory']) # To copy the whole directory shutil.copytree( str(benchmark_outputs), str(context.obj['output_directory'], dirs_exist_ok=True, # python>=3.8, otherwise just call `cp -R` to do it yourself... ) # Otherwise run your code, that create *output.jpg* To actually import the results, create a batch (more info later) for the benchmark. qa batch import-standard-benchmark with: Copy# qa/batches.yamlimport-standard-benchmark: type: benchmark configurations: - benchmark: standard-benchmark inputs: - images Now you can make comparaisons! tipFrom the QA-Board web application, you can set the benchark as a \"milestone\", to compare your results to it in a click. "},{title:"Specifying configurations",type:0,sectionRef:"#",url:"docs/specifying-configurations",content:""},{title:"Specifying configurations",type:1,pageTitle:"Specifying configurations",url:"docs/specifying-configurations#specifying-configurations",content:"You can specify configurations on the CLI: Copyqa --configuration low-power run --input my/test#=> ctx.obj['configuration'] = ['low-power']qa --configuration base:delta run --input my/test#=> ctx.obj['configuration'] = ['base', 'delta'] # Note: The \":\"-separated syntax will be replaced by just giving multiple --configuration flags.# Users usually run batches, and rarely write `qa run` commands by hand. If you use batches: Copy# qa/batches.yamlmy-batch: inputs: - A.jpg configurations: - base - delta # $ qa batch my-batch# => qa --configuration base:delta run A.jpg# => qa --configuration base:delta run B.jpg "},{title:"Common meaning for configurations",type:1,pageTitle:"Specifying configurations",url:"docs/specifying-configurations#common-meaning-for-configurations",content:'While QA-Board is not opiniated, projects usually consider that each configuration in ctx.obj["configurations"] is meant to be merged with ones before. Using "delta"/"cascading"/"partial" configurations is easy to work with. standardize on setups like: Copy# ctx.obj[\'configuration\'] as YAML:configurations:- base # load from a file, e.g. ./configs/{base}.yaml, kept in source control- /abs/path/to/config.yaml # read from absolute paths for convenience- key: value # give directly parameters...- section: # don\'t be shy to structure parameters! key2: value2 You are free to pick different conventions. API DesignToday the API provides tuning parameters via extra_parameters, as a dict... In the future we may simply append it to ctx.obj[\'configurations\'], to let users transparently do tuning.  '},{title:"Use-case #1: Running Python code",type:1,pageTitle:"Specifying configurations",url:"docs/specifying-configurations#use-case-1-running-python-code",content:'Copyfrom pathlib import Pathimport yaml def run(): parameters = {} for c in context.obj["configurations"]: if isinstance(c, str): # Load from a file. # Supports absolute paths for free config_path = Path(\'configurations\') / f"{c}.yaml" with config_path.open() as f: new_parameters = yaml.load(f) if isinstance(c, dict): new_parameters = c # Maybe you will prefer deep-merges parameters.update(new_parameters) if context.obj["extra_parameters"]: parameters.update(context.obj["extra_parameters"]) return my_custom_run( input=context.obj["absolute_input_path"], output=context.obj["output_directory"], parameters=parameters ) '},{title:"Use-case #2: Running an executable",type:1,pageTitle:"Specifying configurations",url:"docs/specifying-configurations#use-case-2-running-an-executable",content:"It could work as before with Copy # --snip-- config_path = context.obj[\"output_directory\"] / \"config.yaml\" with config_path.open('w') as f: yaml.dump(parameters, f) # --snip-- command = [ # ... '--configuration', str(config_path), # ... ] You could also parse the dicts to add CLI parameters... Whatever works for you! "},{title:"Starting a QA-Board server",type:0,sectionRef:"#",url:"docs/start-server",content:""},{title:"Starting the server",type:1,pageTitle:"Starting a QA-Board server",url:"docs/start-server#starting-the-server",content:"For now you'll have to follow the instructions here https://github.com/Samsung/qaboard/tree/master/backend It will be made simpler very soon...!Please fill issues, chat or send an email to maintainers if you run into issues.  noteWe're considering offering a hosted solution to help users get started. If your're interested, contact the maintainers.  "},{title:"Triggering CI and third-party tools via the web application",type:0,sectionRef:"#",url:"docs/triggering-third-party-tools",content:""},{title:"Adding badges and external links",type:1,pageTitle:"Triggering CI and third-party tools via the web application",url:"docs/triggering-third-party-tools#adding-badges-and-external-links",content:"Configure your project's qaboard.yaml like so to display direct links to docs, build artifacts, coverage reports, etc: Copyintegrations:- text: Docs href: http://my-project/docs - src: https://gitlab.com/my/project/badges/develop/coverage.svg href: http://my-project/docs alt: Coverage Report tipThe menu item will be disabled if the link doesn't work. To show a link but run the check on an other URL, you can provide url, method (POST..), etc. If you add allow_failed: true the link is always enabled. "},{title:"Play GitlabCI manual jobs",type:1,pageTitle:"Triggering CI and third-party tools via the web application",url:"docs/triggering-third-party-tools#play-gitlabci-manual-jobs",content:"Configure your project's qaboard.yaml: Copyintegrations: - text: Gitlab Job gitlabCI: job_name: build-linux  "},{title:"Trigger Jenkins builds",type:1,pageTitle:"Triggering CI and third-party tools via the web application",url:"docs/triggering-third-party-tools#trigger-jenkins-builds",content:'Configure your project\'s qaboard.yaml: Copyintegrations: - text: Jenkins Triggered Build jenkins: build_url: $JENKINS_URL/job/CDE_Project_Linux parameters: commit: "${commit.id}" '},{title:"Using webhooks",type:1,pageTitle:"Triggering CI and third-party tools via the web application",url:"docs/triggering-third-party-tools#using-webhooks",content:'You can use webhooks to trigger a variety of external tools: Copyintegrations: - text: Jenkins Triggered Build webhook: - text: \'Windows\', icon: build webhook: # all the options are send straight to the axios http library. For reference: # https://github.com/axios/axios#axios-api - url: "https://my-application/${project}" method: POST data: branch: "${commit.branch}" '},{title:"Using variables",type:1,pageTitle:"Triggering CI and third-party tools via the web application",url:"docs/triggering-third-party-tools#using-variables",content:"You can use some special variables in your strings with some ${VARIABLE} templating: Commit: commit.id, commit.branch... Also branch.Project: project (full project name), subproject (project name relative to the root project), Git repository data with git: eg git.default_branch... Artifacts are saved under commit.commit_dir_url = commit.repo_commit_dir_url / subproject.user is the one defined in the tuning tab or the project's default.  tipIf you use use ${branch} in any of the fields, the integration will only appear on project/branch pages. You can add a dummy only: {branch}. "},{title:"Styling the integrations",type:1,pageTitle:"Triggering CI and third-party tools via the web application",url:"docs/triggering-third-party-tools#styling-the-integrations",content:"Optionnaly you can style each menu item: Copyintegrations: - text: Styled Integration # Full list of icons: https://blueprintjs.com/docs/#icons icon: build # Full list of options: https://blueprintjs.com/docs/#core/components/menu label: docs disabled: false intent: danger You can add dividers to group integration: Copyintegrations: # --snip-- - divider: true title: Section Title # --snip-- "},{title:"Example: Jenkins integration via Webhooks",type:1,pageTitle:"Triggering CI and third-party tools via the web application",url:"docs/triggering-third-party-tools#example-jenkins-integration-via-webhooks",content:'cautionThe out-of-the-box jenkins integration above is much better! This is just an example with webhooks! If you don\'t have one, get an API token for your user Copy$JENKINS_URL/me/descriptorByName/jenkins.security.ApiTokenProperty/generateNewToken# Enter "OK to retry using POST" and get the "tokenValue" cautionSince you\'ll commit those credentials with the code, make sure you don\'t have too many privileges... At some point qatools will support secrets.  Get a crumb to handle Jenkins\' CSRF, eg at $JENKINS_URL/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,%22:%22,//crumb)Go to your Jenkins project configuration page at $JENKINS_URL/$PROJECT/configure and allow triggered builds:Configure your project\'s qaboard.yaml: Copyintegrations: - name: Jenkins Triggered Build webhook: method: post url: $JENKINS_URL/job/$PROJECT/buildWithParameters headers: Jenkins-Crumb: $JenkinsCrumb params: token: $TOKEN auth: { username: $username password: api-token # keep as-is data: commit: "${commit.id}" cause: Triggered on the QA web app Alternatively, you can also send users to the build page:  Copyintegrations: - text: Build label: With Parameters icon: build href: $JENKINS_URL/view/HW_ALG/job/HW_ALG-delivery/build?delay=0sec # Jenkins behaves wtf and returns 405 errors... # https://issues.jenkins-ci.org/browse/JENKINS-3121 ignore_failure: true '},{title:"Enabling tuning and extra runs from QA-Board",type:0,sectionRef:"#",url:"docs/tuning-from-the-webapp",content:""},{title:"How to do tuning or trigger extra runs from QA-Board",type:1,pageTitle:"Enabling tuning and extra runs from QA-Board",url:"docs/tuning-from-the-webapp#how-to-do-tuning-or-trigger-extra-runs-from-qa-board",content:"When doing QA or during development, you often want to run the code/configs from a given commit on new tests. QA-Board lets you define and runs batches of tests with extra tuning parameters:  cautionRight now, it doesn't work (outside of Samsung....) as the server is hardcoded to use our infra. It will change very soon! "},{title:"Enabling tuning from QA-Board",type:1,pageTitle:"Enabling tuning and extra runs from QA-Board",url:"docs/tuning-from-the-webapp#enabling-tuning-from-qa-board",content:""},{title:"Task runner",type:1,pageTitle:"Enabling tuning and extra runs from QA-Board",url:"docs/tuning-from-the-webapp#task-runner",content:"You need to configure a task runner, that will execute tuning runs asynchronously. We recommend getting started with Celery. All the details are on the next page! "},{title:"Build Artifacts",type:1,pageTitle:"Enabling tuning and extra runs from QA-Board",url:"docs/tuning-from-the-webapp#build-artifacts",content:'Define artifacts: you must define the "artifacts" needed to run your software. Besides the source, you might need compiled binaries, configurations, trained networks, etc. Artifacts are defined in qaboard.yaml: Copyartifacts: binary: glob: \'build/sample_project\' # The "configurations" artifacts are shown in the UI under the commit\'s "Configuration" tab configurations: glob: configurations/*.json For convenience, .qaboard.yaml and qatools/ are saved automatically. Save the artifacts when your build/training is done. In your CI, you will want to execute: Copyqa save-artifacts '},{title:"Handling tuning parameters",type:1,pageTitle:"Enabling tuning and extra runs from QA-Board",url:"docs/tuning-from-the-webapp#handling-tuning-parameters",content:"You entrypint's run() function should do something with context.obj['extra_parameters']`. That's all. "},{title:"Various Tuning Workflows",type:0,sectionRef:"#",url:"docs/tuning-workflows",content:""},{title:"Tuning from QA-Board",type:1,pageTitle:"Various Tuning Workflows",url:"docs/tuning-workflows#tuning-from-qa-board",content:"When doing QA or during development, you often want to run the code/configs from a given commit on new tests. QA-Board lets you define and runs batches of tests with extra tuning parameters:  "},{title:"Investigating results/configs you see in the UI",type:1,pageTitle:"Various Tuning Workflows",url:"docs/tuning-workflows#investigating-resultsconfigs-you-see-in-the-ui",content:"Every time you see an output in the web application, you see what configurations were used, and you can easily open the output directory:   The output logs always show you the exact CLI commands that were used, so that reproducing results is only a git checkout $revision ; make ; qa run away. "},{title:"Workflows used for Tuning",type:1,pageTitle:"Various Tuning Workflows",url:"docs/tuning-workflows#workflows-used-for-tuning",content:""},{title:"Local Workflow",type:1,pageTitle:"Various Tuning Workflows",url:"docs/tuning-workflows#local-workflow",content:"If you already have great development/debugging tools, use them! At SIRC, CDE provides a great environment to run hardware chains and view images.**For deep learning tensorboard is a great tool to investigate NNs.Many people love to write one-off matlab script. You can continue to use the existing tools! This said, it's worth having your IDE/debugger/scripts call your code via qatools' CLI. Here is how to do it. "},{title:"Local configs > SharedStorage > Tuning from QA-Board Workflow",type:1,pageTitle:"Various Tuning Workflows",url:"docs/tuning-workflows#local-configs--sharedstorage--tuning-from-qa-board-workflow",content:"Details: WIP "},{title:"Local > QA-Board Workflow",type:1,pageTitle:"Various Tuning Workflows",url:"docs/tuning-workflows#local--qa-board-workflow",content:"QA-Board lets you runs your local code/configurations, and see results in the web application. It gives you an easy way to tweak/compile/run your code and compare results across runs: Copyqa --share run [...]qa --share --label testing-some-logic-tweaks batch [...] Results will appear in a new batch:   "},{title:"Commit > CI > QA-Board Qorkflow",type:1,pageTitle:"Various Tuning Workflows",url:"docs/tuning-workflows#commit--ci--qa-board-qorkflow",content:"If you make changes in configuration files, you need to commit them. 1. Make changes 2. Commit the changes 3. Push your commit 4. See results in the UI "},{title:"Tips for CLI usage",type:0,sectionRef:"#",url:"docs/using-the-qa-cli",content:""},{title:"CLI flags worth knowing",type:1,pageTitle:"Tips for CLI usage",url:"docs/using-the-qa-cli#cli-flags-worth-knowing",content:""},{title:"qa --help",type:1,pageTitle:"Tips for CLI usage",url:"docs/using-the-qa-cli#qa---help",content:"All commands have some help: Copyqa --helpqa batch --help "},{title:"qa --share",type:1,pageTitle:"Tips for CLI usage",url:"docs/using-the-qa-cli#qa---share",content:'When you run qa batch or qa run on your terminal, results are saved locally under output/, and they are not visible in QA-Board. To make them visible: tipIf you don\'t like this default, make --share the default via Copy# .bashrc or other shell configalias qa="qa --share" # you can also use an environment variableexport QATOOLS_SHARE=true '},{title:"qa --dryrun",type:1,pageTitle:"Tips for CLI usage",url:"docs/using-the-qa-cli#qa---dryrun",content:'qa commmands support a --dryrun mode, where they print actions they would take, but don\'t actually do anything. In particular it helps see quickly what inputs you defined in a batch: Copyqa --dryrun batch my-batch# qa run --input image/A.jpg# qa run --input image/B.jpg noteFor qa --dryrun run, you are expected to handle if context.obj["dryrun"]: ... yourself in run(). The use-case is usually printing how you would call an executable, for debugging. '},{title:"qa --label my-label",type:1,pageTitle:"Tips for CLI usage",url:"docs/using-the-qa-cli#qa---label-my-label",content:'Everytime you qa run, it erases previous results. So if you want compare different versions by tweaking doing qa run, it won\'t work. Fortunately, qa lets you give a "label", or "experiment name" to runs. Results with different labels are stored separately: Copyqa --label without-optimizations batch validation-imagesqa --label with-optimizations batch validation-images  tipTo keep previous output files, export QATOOLS_RUN_KEEP=true. It can be useful if you are debugging long runs and implemented a caching mecanism. (Experimental) '},{title:"qa batch",type:1,pageTitle:"Tips for CLI usage",url:"docs/using-the-qa-cli#qa-batch",content:""},{title:"Connecting to a custom QA-Board instance",type:1,pageTitle:"Tips for CLI usage",url:"docs/using-the-qa-cli#connecting-to-a-custom-qa-board-instance",content:"Use qa --offline to ensure you don't connect to a QA-Board instance. It's useful if... you don't have one (?). The default connection settings can be overriden by environment variables. For example: Copyexport QATOOLS_DB_PROTOCOL=httpexport QATOOLS_DB_HOST=qaexport QATOOLS_DB_PORT=5000 "},{title:"Batch Runners",type:1,pageTitle:"Tips for CLI usage",url:"docs/using-the-qa-cli#batch-runners",content:"While qa run uses the local environment, qa batch will offload computation to a \"runner\" backend. Currently: On Windows we use joblib for parallel computing. You can set the concurrency with QATOOLS_BATCH_CONCURRENCY and other environment variables from joblib. runners.local.concurrency in qaboard.yaml also works...On linux we use SIRC's LSF cluster You can also set the runner via --runner=local, and even set a default with runners.default: local in qaboard.yaml. Help needed for more runner!We intent on supporting more task runners: python-rq or celery, maybe even a custom one with just list of hosts to ssh into... Ideally we'll implement a couple integrations, then write integration docs and rely on the community. Maybe we can piggyback on joblib if other project provide distributed backend... "},{title:"Dealing with existing results",type:1,pageTitle:"Tips for CLI usage",url:"docs/using-the-qa-cli#dealing-with-existing-results",content:"When you try to re-run already existing results, The behaviour of qa batch can be changed with the --action-on-existing flag: --action-on-existing=run: overwrite the old results (default).postprocess: only call the postprocess() function, not run()+postprocess() as usual. (Note: it's also provided by qa postprocess)sync: update the output file manifest and read metrics from $output_dir/metrics.json. (Note: it's also provided by qa sync)skip: do nothing  "},{title:"Visualizing your algorithm's outputs",type:0,sectionRef:"#",url:"docs/visualizations",content:""},{title:"Available file viewers",type:1,pageTitle:"Visualizing your algorithm's outputs",url:"docs/visualizations#available-file-viewers",content:"QA-Board tries to guess the right image viewer depending on the file extension or a type  Extenstions Type Viewer *.jpg* , *.png* , *.bmp* , *.tif* , *.pdf* ... image/* Image *.hex* , *.raw* , *.dng* image/* Image (via CDE) *.plotly.json plotly/json Plot.ly *.mp4 video/* Video (synced) *.html plain/html HTML (assumes trusted input..!) *.txt , unidentified text/plain* Text (diffs, with VSCode&apos;s Monaco Editor ) &lt;/&gt; pointcloud/txt ToF&apos;s pointcloud viewer (needs to be split) &lt;/&gt; 6dof/txt SLAM&apos;s 6DoF viewer (needs to be split)  "},{title:"Dynamic visualizations",type:1,pageTitle:"Visualizing your algorithm's outputs",url:"docs/visualizations#dynamic-visualizations",content:'You can use a special syntax to create dynamic visualizations at display-time. Users will we able to choose what to display using sliders / select options:  Copyoutputs: visualizations: - name: Movie Frames # you can use the `/user/:name` syntax to match part of filenames path: ":frame/output.jpg" # you can match part of filenames (experimental) path: ":frame/frame_:number.jpg" # For more examples, the full syntax is available at: # https://github.com/pillarjs/path-to-regexp  You can also use regular expressions (inside parentheses!) to match which output files you want to view: Copyoutputs: visualizations: # A common use case is matching file extensions path: "(.*\\.jpg)" # ... or parts of filenames path: "(debug_.*\\.jpg)" # you can mix with the previous syntax path: ":frame/(.*\\.txt)" # If you use regular expressions, we aware that: # - You MUST use "( )" aka "capture groups" ! # - While you can often get away "(.*)/output.jpg", in many cases you\'d want "([^/]*)/output.jpg" # - Parts of paths matched via regular expressions are not synced with other outputs. Prefer the ":name" syntax # Eg if you ask also to visualize "(.*)/debug_output.jpg" and "(.*)/output.jpg" # you will get two select inputs for the frame. By default, only one viewer/path is shown at a time, and you get sliders/select to decide what to show:  If you want, you can visualize all matching files: Copy # --snip-- - name: KPI reports path: "reports/:report" type: plotly/json display: single # (default): will list views one after the other # all # will render all matching paths/views # viewer # let the viewer decide what to do... (EXPERIMENTAL) '},{title:"Advanced Options [EXPERIMENTAL]",type:1,pageTitle:"Visualizing your algorithm's outputs",url:"docs/visualizations#advanced-options-experimental",content:""},{title:"Image viewer",type:1,pageTitle:"Visualizing your algorithm's outputs",url:"docs/visualizations#image-viewer",content:"Supports all common image formats.Fast and smooth zoom & pan, synced. Fast image streaming via IIIF.Color tooltip.Perceptual color difference.Automatic regions of interest.Image filters (exposure, contrast, gamma...).Histograms per channel.  "},{title:"Plot.ly viewer",type:1,pageTitle:"Visualizing your algorithm's outputs",url:"docs/visualizations#plotly-viewer",content:"The Plotly library has everything you need from bar charts to 3d plots. huge variety of plotsinteractive plotseasy-ish to use with binding to python/JS/matlab...web-basedopen-source and popularperformant   All you need is to save your plot data as JSON.  Copyimport plotly.graph_objects as gofig = go.Figure(data=go.Bar(y=[2, 3, 1]))with open('graph.plotly.json', 'w') as f: spec = fig.to_json() # '{\"layout\": {...}, \"data\": [{...}, {...}, ...]}' f.write() "},{title:"Text Viewer",type:1,pageTitle:"Visualizing your algorithm's outputs",url:"docs/visualizations#text-viewer",content:" "},{title:"More Viewers?",type:1,pageTitle:"Visualizing your algorithm's outputs",url:"docs/visualizations#more-viewers",content:"Contact us to tell us what you need! The backlog contains: Flame Graphs and differential-flame-graphs for software performanceConfig CDE: as graph like tensorboardVega "},{title:"Custom Styles",type:1,pageTitle:"Visualizing your algorithm's outputs",url:"docs/visualizations#custom-styles",content:"You can style your visualizations: Copyoutputs: # define global or per-view styles style: # use any CSS properties width: 500px # the style will be applied to the outer-container # and passed down to the viewers detailed_views: - name: My debug visualization style: width: 400px  "},{title:"Viewer Configuration",type:1,pageTitle:"Visualizing your algorithm's outputs",url:"docs/visualizations#viewer-configuration",content:'Some viewers can read extra configuration parameters from their configuration:  Copyoutputs: detailed_views: - name: My SLAM plot type: 6dof/txt show_3d: true You can specify those parameters at "display-time" by defining controls: Copyoutputs: controls: - type: toggle label: Debug name: show_debug default: false '}]}}]);